{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f8efb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd # data manipulation\n",
    "import alpaca_trade_api as tradeapi\n",
    "import matplotlib.pyplot as plt\n",
    "from MCForecastTools import MCSimulation\n",
    "import plotly.graph_objects as go\n",
    "import panel as pn\n",
    "%matplotlib inline\n",
    "pn.extension('plotly','tabulator' , 'echarts')\n",
    "\n",
    "from PIL import Image\n",
    "from html2image import Html2Image\n",
    "import param \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# loading the reqed libraries for scraping from Twitter and Reddit \n",
    "import tweepy # twitter api module \n",
    "import plotly.express as px \n",
    "import os\n",
    "import hvplot.pandas # plotting and visuliaizng the data \n",
    "import praw # reddit api module \n",
    "import os  # os module to access the .env file \n",
    "from dotenv import load_dotenv \n",
    "import torch # required library for transformers models \n",
    "\n",
    "from bs4 import BeautifulSoup as bs # scraping and html parsing for some sites\n",
    "import requests as rq # required for scraping from finviz.com\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification # NLP \n",
    "TOKENIZERS_PARALLELISM=False\n",
    "import json # used to parse the http request data from finviz or cryptopanic\n",
    "# easier way to scrape from finviz.com \n",
    "import finviz\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_agg import FigureCanvas\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib as mpl\n",
    "from lxml import etree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63e2d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = pn.widgets.TextInput(name='Asset', value=\"AAPL\" , placeholder = \"enter Asset name \")\n",
    "toggle = pn.widgets.ToggleGroup(options=['Stocks', 'Crypto'], behavior='radio', button_type=\"default\")\n",
    "no_tweets = pn.widgets.TextInput(name='Number of Tweets', value=\"100\" , placeholder = \"enter Asset name \" , max_length =3 )\n",
    "button = pn.widgets.Button(name='Submit')\n",
    "#info = pn.widgets.StaticText(value=df_metrics_long['value'][3])\n",
    "\n",
    "load_dotenv()\n",
    "AlphaVantageKey = os.getenv(\"AlphaVantageKey\")\n",
    "\n",
    "\n",
    "url = 'https://www.alphavantage.co/query?function=OVERVIEW&symbol=' + str(text_field.param.value) +'&apikey=' + str(AlphaVantageKey)\n",
    "r = rq.get(url)\n",
    "stock_metrics = r.json()\n",
    "df_metrics = pd.DataFrame(stock_metrics,index = range(1))\n",
    "df_metrics_long = df_metrics.melt()\n",
    "metric_table = pn.widgets.Tabulator(df_metrics_long, width=1000)\n",
    "#info = pn.widgets.StaticText(value=df_metrics_long['value'][3])\n",
    "@pn.depends(text_field.param.value,no_tweets.param.value)\n",
    "def tabs(text_field,no_tweets):\n",
    "    \n",
    "    \"\"\"Twitter Sentiment Data Analysis\"\"\"\n",
    "    \n",
    "    #create the twitter api access varibles \n",
    "    twit_api= os.getenv(\"TWIT_API\")\n",
    "    twit_secret = os.getenv(\"TWIT_SECRET\")\n",
    "    access_token = os.getenv(\"ACCESS_TOKEN\")\n",
    "    access_token_secret = os.getenv(\"ACCESS_TOKEN_SECRET\")\n",
    "    # authentication for the twitter api \n",
    "    auth = tweepy.OAuthHandler(twit_api, twit_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    #createing the API object\n",
    "    t_api = tweepy.API(auth)\n",
    "    #createing the API object\n",
    "    t_api = tweepy.API(auth)\n",
    "    # search tweets\n",
    "    keyword = f\"${text_field}\"\n",
    "    tweets = tweepy.Cursor(t_api.search_tweets, q=keyword, count=int(no_tweets), tweet_mode='extended').items(int(no_tweets))\n",
    "    # create DataFrame\n",
    "    columns = ['ID','User', 'Tweet' , 'Time']\n",
    "    data = []\n",
    "    for tweet in tweets:\n",
    "        data.append([tweet.id , tweet.user.screen_name, tweet.full_text , tweet.user.created_at])\n",
    "        # clean the tweets from the RT and and @user calls using the lambda function in a two step process\n",
    "\n",
    "    tweet_list = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    pattern = '(RT @)\\w+:'\n",
    "    tweet_list['Tweet'] =tweet_list['Tweet'].apply(lambda x: re.sub(pattern , '' ,x))\n",
    "    pattern = '(@)\\w+'\n",
    "    tweet_list['Tweet'] =tweet_list['Tweet'].apply(lambda x: re.sub(pattern , '' ,x))\n",
    "        # create tokenizer variable to chose the model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        # encode the list of tweet to create the sentiment score from a function\n",
    "    def sentiment_score(tweet):\n",
    "        tokens = tokenizer.encode(tweet, return_tensors='pt')\n",
    "        result = model(tokens)\n",
    "        return int(torch.argmax(result.logits))+1\n",
    "    tweet_list['sentiment'] = tweet_list['Tweet'].apply(lambda x: sentiment_score(x[:]))\n",
    "        # do some data manipulation to work out some cool statistics and visuals (more TBD)\n",
    "    tweet_list.drop_duplicates(inplace=True)\n",
    "    tweet_list.dropna(inplace=True)\n",
    "    # add year , month and day to the dataset\n",
    "    tweet_list['year']= tweet_list['Time'].dt.year\n",
    "    tweet_list['month']= tweet_list['Time'].dt.month\n",
    "    tweet_list['day']= tweet_list['Time'].dt.day\n",
    "    #set the index to those columns \n",
    "    tweet_list.set_index(['year','month','day'] , inplace=True)\n",
    "       \n",
    "       \n",
    "    #create a count of sentiment and persentage for pie charting\n",
    "    sentiment_count = tweet_list['sentiment'].value_counts()\n",
    "    #sentiment_count.to_frame()\n",
    "    sentiment_count.sort_index(inplace=True)\n",
    "    \n",
    "    #get 10 positive and negative tweets \n",
    "    largest = tweet_list.nlargest(10, columns='sentiment').reset_index()\n",
    "    pd.to_datetime(largest['Time'] , format='%d/%m/%y')\n",
    "    largest_sen_tweets = largest[['Time','User','Tweet' , 'sentiment']]\n",
    "    smallest = tweet_list.nsmallest(10, columns = 'sentiment').reset_index()\n",
    "    pd.to_datetime(smallest['Time'] , format='%d/%m/%y')\n",
    "    smallest_sen_tweets = smallest[['Time','User','Tweet','sentiment']] \n",
    "\n",
    "    \"\"\"\n",
    "    total = sentiment_count.sum()\n",
    "    sentiment_count.get(1)\n",
    "    sentiment_count.get(2)\n",
    "    sentiment_count.get(3)\n",
    "    negative_senti =round((sentiment_count[1]/total)*100,2)\n",
    "    neutral_senti =round((sentiment_count[2]/total)*100,2)\n",
    "    positive_senti =round((sentiment_count[3]/total)*100,2)\n",
    "        \"\"\"\n",
    "    #Create a pie chart and store the object in pie_pane\n",
    "    fig0 = Figure(figsize=(4,4))\n",
    "    ax0 = fig0.subplots()\n",
    "    pie = ax0.pie(sentiment_count,\n",
    "                  labels=['Negative','Neutral','Positive'],\n",
    "                  autopct = '%.2f',\n",
    "                  shadow=True,\n",
    "                  startangle=90)\n",
    "    \n",
    "    pie_pane = pn.pane.Matplotlib(fig0, dpi=144 , tight = True)\n",
    "    \n",
    "    #group_tweet_senti = tweet_list.groupby(by='sentiment').count()\n",
    "    #group_tweet_senti.drop(columns=['Time','User'] ,inplace=True)\n",
    "    \n",
    "    #plot = group_tweet_senti.hvplot.bar(x='sentiment')\n",
    "    #plot2 = group_tweet_senti.hvplot.hist(y='User')\n",
    "    #plot3= group_tweet_senti.hvplot.hist(y='Tweet')\n",
    "    \n",
    "    #Hist_plots = pn.Column(plot,plot2,plot3)\n",
    "    \n",
    "    \"\"\"Consumer Sentiment Word Plot results.\"\"\"\n",
    "\n",
    "\n",
    "    # CODE HERE\n",
    "    text = ' '.join(tweet_list['Tweet'])\n",
    "\n",
    "    comment_words = \"\"\n",
    "    stopwords = set(STOPWORDS)\n",
    "    #stopwords = set(STOPWORDS)\n",
    "    custom_stop_words = [\"https\" , 't', 'co' , 'do' , 'yet']\n",
    "    [stopwords.add(n) for n in custom_stop_words]\n",
    "    \n",
    "    wc = WordCloud(background_color=\"white\" , width=500, max_words=3500, \n",
    "              stopwords=stopwords, max_font_size=100, random_state=42)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    mpl.rcParams[\"figure.figsize\"]=[10.0,5.0]\n",
    "    #generate word cloud\n",
    "    wc_image= wc.generate(text)\n",
    "    plt.imshow(wc_image)\n",
    "    plt.axis(\"off\")\n",
    "    fontdict={\"fontsize\": 48, \"fontweight\":\"bold\"}\n",
    "    plt.title(f\"{text_field} Word Cloud\", fontdict=fontdict)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \"\"\"Get Stock price data and tabulate metrics\"\"\"\n",
    "    \n",
    "    # Read API keys into env \n",
    "    # Read the API keys\n",
    "    load_dotenv()\n",
    "    AlphaVantageKey = os.getenv(\"AlphaVantageKey\")\n",
    "    \n",
    "    # Set Alpaca API key and secret\n",
    "    alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "    alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "    \n",
    "    api = tradeapi.REST(\n",
    "        alpaca_api_key,\n",
    "        alpaca_secret_key,\n",
    "        api_version = \"v2\"\n",
    "    )\n",
    "    \n",
    "    #Get stock data\n",
    "    # Set timeframe to '1D'\n",
    "    timeframe = \"1D\"\n",
    "    \n",
    "    # Set start and end datetimes between now and 3 years ago.\n",
    "    start_date = pd.Timestamp(\"2018-05-01\", tz=\"America/New_York\").isoformat()\n",
    "    end_date = pd.Timestamp(\"2022-01-19\", tz=\"America/New_York\").isoformat()\n",
    "    \n",
    "    # Set the ticker information\n",
    "    tickers = text_field\n",
    "    \n",
    "    # Get 3 year's worth of historical price data\n",
    "    \n",
    "    df_ticker = api.get_barset(\n",
    "        tickers,\n",
    "        timeframe,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        limit=1000,\n",
    "    ).df\n",
    "        \n",
    "        \n",
    "    #initilize some varibles \n",
    "    ema = True\n",
    "    periods = 14\n",
    "    close_delta = df_ticker.iloc[:,3].diff()\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    if ema == True:\n",
    "    # Use exponential moving average\n",
    "        ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "        ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    else:\n",
    "        # Use simple moving average\n",
    "        ma_up = up.rolling(window = periods, adjust=False).mean()\n",
    "        ma_down = down.rolling(window = periods, adjust=False).mean()\n",
    "        \n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    \n",
    "    df_ticker['RSI'] = rsi\n",
    "    df_ticker['RSI'].dropna().head(20)\n",
    "    df_ticker['RSI UCL'] = 70\n",
    "    df_ticker['RSI LCL'] = 30\n",
    "    \n",
    "    #rsi_plot = px.line(x=df_ticker.index,y = df_ticker.RSI, title='RSI Chart')\n",
    "    #rsi_plot.add_scatter(x=df_ticker.index,y = df_ticker['RSI UCL'],name='BUY')\n",
    "    #rsi_plot.add_scatter(x=df_ticker.index,y = df_ticker['RSI LCL'],name='SELL')\n",
    "    \n",
    "    # calculate Bolinger Bands \n",
    "    df_ticker['SMA']=df_ticker.iloc[:,3].rolling(window=13).mean()\n",
    "    bolly_band_std=df_ticker.iloc[:,3].rolling(window=13).std()\n",
    "    df_ticker['UB']=df_ticker['SMA']+(bolly_band_std*2)\n",
    "    df_ticker['LB']=df_ticker['SMA']-(bolly_band_std*2)\n",
    "    \n",
    "    df_ticker.dropna(inplace=True)\n",
    "    \n",
    "    #create candelstick chart \n",
    "    url = 'https://www.alphavantage.co/query?function=OVERVIEW&symbol=' + str(tickers) +'&apikey=' + str(AlphaVantageKey)\n",
    "    r = rq.get(url)\n",
    "    stock_metrics = r.json()\n",
    "    df_metrics = pd.DataFrame(stock_metrics,index = range(1))\n",
    "    df_metrics_long = df_metrics.melt()\n",
    "    metric_table = pn.widgets.Tabulator(df_metrics_long, width=300 , show_index=False)\n",
    "    \n",
    "    ## news feed from finviz to display\n",
    "    news = finviz.get_news(text_field)\n",
    "    news_cols = ['Time','News','url','Source',] \n",
    "    df_news = pd.DataFrame(news , columns=news_cols)\n",
    "    df_news.drop(columns=['url'] , inplace=True)\n",
    "    df_news = df_news.reindex(columns=['News','Source','Time'])\n",
    "    df_news\n",
    "    news_pane = pn.widgets.Tabulator(df_news, width=1000 , show_index=False)\n",
    "    \n",
    "    final_fig = make_subplots(rows=2 , cols =1)\n",
    "    \n",
    "    final_fig.append_trace(go.Candlestick(\n",
    "                                            x=df_ticker.index,\n",
    "                                            open=df_ticker[tickers,\"open\"],\n",
    "                                            high=df_ticker[tickers,\"high\"],\n",
    "                                            low=df_ticker[tickers,\"low\"],\n",
    "                                            close=df_ticker[tickers,\"close\"]\n",
    "                                            \n",
    "                                        ),\n",
    "                        row = 1, col=1)\n",
    "    \n",
    "    final_fig.append_trace(go.Scatter(x=df_ticker.index , y = df_ticker['RSI']), row=2,col=1)\n",
    "    \n",
    "    \n",
    "    final_fig.update_layout(title= f\"Price Chart for {text_field}\")\n",
    "    final_fig.update_traces(name='Price', selector=dict(type='candlestick'))\n",
    "    final_fig.update_traces(name='RSI', selector=dict(type='Scatter'))\n",
    "    final_fig.update_layout(xaxis_rangeslider_visible=False)\n",
    "    #final_fig.update_layout(uirevision=df_ticker)\n",
    "    \"\"\"\n",
    "    fig = go.Figure(data=[go.Candlestick(\n",
    "                                        x=df_ticker.index,\n",
    "                                        open=df_ticker[tickers,\"open\"],\n",
    "                                        high=df_ticker[tickers,\"high\"],\n",
    "                                        low=df_ticker[tickers,\"low\"],\n",
    "                                        close=df_ticker[tickers,\"close\"]\n",
    "                                    )\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "    fig.add_trace(go.Line(x=df_ticker.index,y = df_ticker['UB'],name='Upper Band'))\n",
    "    fig.add_trace(go.Line(x=df_ticker.index,y = df_ticker['LB'],name='Lower Band'))\n",
    "    #fig2=go.Figure(data=[go.Scatter(x=df_ticker.index , y = df_ticker['RSI'],name='RSI')])\n",
    "    fig.add_trace(go.Line(x=df_ticker.index,y = df_ticker['RSI'],name='RSI'))\n",
    "    \"\"\" \n",
    "    \"\"\" Creating the Simulation Plots \"\"\"\n",
    "    \n",
    "    df_stock_data = df_ticker[tickers,\"close\"]\n",
    "    stock_close_df = df_stock_data.to_frame()\n",
    "    \n",
    "    MC_thirtyyear = MCSimulation(\n",
    "    portfolio_data = stock_close_df,\n",
    "    weights = [1],\n",
    "    num_simulation = 10,\n",
    "    num_trading_days = 252*30)\n",
    "    \n",
    "    MC_thirtyyear.calc_cumulative_return()\n",
    "    \n",
    "    simulation_df, plot_title = MC_thirtyyear.plot_simulation()\n",
    "    line_plot = simulation_df.hvplot(title=plot_title, legend=False).opts(yformatter=\"%.0f\")\n",
    "      \n",
    "    \"\"\"Fear & Greed Index results.\"\"\"\n",
    "\n",
    "    #hti = Html2Image()\n",
    "    #url_data = 'https://money.cnn.com/data/fear-and-greed/'\n",
    "    #hti.screenshot(url='https://money.cnn.com/data/fear-and-greed/', save_as='fear_reed.png')\n",
    "    ## Importing Image class from PIL modul\n",
    "    ## Opens a image in RGB mode\n",
    "    #im = Image.open(r\"fear_reed.png\")\n",
    "    ## Setting the points for cropped image\n",
    "    #left = 470\n",
    "    #top = 80\n",
    "    #right = 1100\n",
    "    #bottom = 400\n",
    "    ## Cropped image of above dimension\n",
    "    ## (It will not change original image)\n",
    "    #im1 = im.crop((left, top, right, bottom))\n",
    "    ## Shows the image in image viewer\n",
    "    #im1.save(\"pic.png\" , format=\"png\")\n",
    "    #fng_fig = pn.pane.PNG(\"pic.png\",alt_text='F&G' , width = 600)\n",
    "\n",
    "    #get the url to scrape from cnn\n",
    "    url = 'https://money.cnn.com/data/fear-and-greed'\n",
    "    res = rq.get(url)\n",
    "    #create a soup object to parse the page\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    # create an etreee object to use Xapth attribute\n",
    "    object_tree = etree.HTML(str(soup))\n",
    "\n",
    "    # create and empty list to store the data being scraped\n",
    "    list_fng = []\n",
    "\n",
    "    # loop over the data and store in the list , this will grab the fear and greed tabulated data from the page\n",
    "    for i in range(5):\n",
    "        list_fng.append([object_tree.xpath('/html/body/div[3]/div[1]/div[1]/div[3]/div/div[1]')[0][0][i].text])\n",
    "\n",
    "    df_fng = pd.DataFrame(list_fng)\n",
    "    for i in df_fng:\n",
    "        df_fng['Value'] = df_fng[i].str.extract(pat ='([0-9]{2})')\n",
    "        df_fng['Status'] = df_fng[i].str.extract(pat= '((?<=\\().+?(?=\\)))')\n",
    "        df_fng['Time'] = df_fng[i].str.replace(pat='(\\:.*$)' , repl = '')\n",
    "\n",
    "    df_fng.drop(columns=0 , inplace=True)\n",
    "\n",
    "    fng_gauge = pn.indicators.Gauge(\n",
    "        name='Fear and Greed Index',\n",
    "        value=int(df_fng['Value'][0]),\n",
    "        bounds=(0, 100), format='{value} %',\n",
    "        colors=[(0.25, 'red'), (0.5, 'orange'), (0.75, 'yellow') , (1 , 'green')])\n",
    "\n",
    "\n",
    "    \"\"\" Bot recomendation based on the metrics\"\"\"\n",
    "    \n",
    "    def bot():\n",
    "        rsi = df_ticker['RSI']\n",
    "        pe = df_metrics_long['value'][15]\n",
    "        fear_greed = df_fng['Value'][0]\n",
    "        \"\"\"\n",
    "        score = 0 \n",
    "        \n",
    "        if rsi > 70:\n",
    "            score+=1\n",
    "        elif rsi <30:\n",
    "            score-=1\n",
    "        else:\n",
    "             score+=0           \n",
    "        if pe > 30:\n",
    "            score-=1\n",
    "        elif pe>15:\n",
    "            score+=1\n",
    "        else:\n",
    "            score+=0            \n",
    "        if fear_greed > 75:\n",
    "            score+=1\n",
    "        elif fear_greed <30:\n",
    "            score-=1\n",
    "        else:\n",
    "            score+=0 \"\"\"\n",
    "        #msg = f'The final score based on the {score} with the follwing paramaters used:\\nRSI was {rsi}\\nPERatio was {pe}\\nFear and Greed Index was {fear_greed}'\n",
    "        \n",
    "        return \"Work in progress to finalize the logic\"\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \"\"\" Layout Design of the dashboard\"\"\"\n",
    "    # Layout design \n",
    "    twitter = pn.Column('#Samples of Tweets' , pn.widgets.Tabulator(largest_sen_tweets,name='High Sentiment Tweets',  show_index = False , row_height = 10),\n",
    "                    pn.widgets.Tabulator(smallest_sen_tweets,  name='Low Sentiment Tweets', show_index = False  , row_height = 10))\n",
    "\n",
    "    fng_word = pn.Column('# Sentiment Graphics' , pn.Row(pie_pane , pn.pane.Matplotlib(fig) , fng_gauge))\n",
    "    \n",
    "    \n",
    "    sentiment = pn.Column(fng_word ,twitter)\n",
    "    \n",
    "    metrics = pn.Column('# Metrics & News ',pn.Row(metric_table , news_pane))\n",
    "     \n",
    "   \n",
    "    Data_panel = pn.Tabs(\n",
    "        ('Charts' , final_fig),\n",
    "        ('Stock Metrics Data' , metrics),\n",
    "        ('Simulation' , line_plot),\n",
    "        ('Sentiment' , sentiment),\n",
    "        ('Recomendation' , bot()))\n",
    "    \n",
    "          \n",
    "    return Data_panel\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "28e785be",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_panel = pn.Column(text_field,no_tweets,button)\n",
    "Complete_dashboard = pn.Row(selection_panel,tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3a4d8f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching server at http://localhost:58647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bokeh.server.server.Server at 0x7fed7f020cd0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "pn.serve(pn.template.VanillaTemplate(\n",
    "    site=\"Group 1\",\n",
    "    title=\"Stock Analysis and Sentiment Dashboard\" ,\n",
    "    main=[\"\"\" \n",
    "    Group 1 Team Members:\\n[Jihad Al-Hussain]\\t[John Gaffney]\\t[Shanel Kuchera]\\t[Kazuki Takehashi]\\t[Patrick Thornquist] \"\"\" , Complete_dashboard]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbeb78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70258ac2307775ea3b2356ae640b2cec78ea29d8e88ec85f576a7eb81644faaf"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pyvizenv] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
