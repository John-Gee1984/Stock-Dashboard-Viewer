{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Light;\f1\fswiss\fcharset0 Helvetica-LightOblique;\f2\fnil\fcharset0 HelveticaNeue-Bold;
\f3\fnil\fcharset0 HelveticaNeue;\f4\fnil\fcharset0 HelveticaNeue-Italic;\f5\fnil\fcharset0 Georgia-Bold;
\f6\fnil\fcharset0 Georgia;\f7\fnil\fcharset0 Georgia-Italic;\f8\fswiss\fcharset0 Helvetica;
\f9\fswiss\fcharset0 Helvetica-Oblique;\f10\fswiss\fcharset0 Helvetica-Bold;\f11\fnil\fcharset0 Menlo-Regular;
\f12\fnil\fcharset0 LucidaGrande;}
{\colortbl;\red255\green255\blue255;\red51\green51\blue51;\red255\green255\blue255;\red216\green0\blue103;
\red42\green43\blue45;\red21\green22\blue24;\red51\green0\blue135;\red20\green20\blue20;\red18\green68\blue139;
\red54\green54\blue54;\red0\green0\blue0;\red101\green175\blue4;\red65\green65\blue65;\red241\green241\blue241;
\red236\green236\blue236;}
{\*\expandedcolortbl;;\cssrgb\c25882\c25882\c25882;\cssrgb\c100000\c100000\c100000;\cssrgb\c88627\c7059\c47843;
\cssrgb\c21569\c22353\c23137;\cssrgb\c10980\c11373\c12549;\cssrgb\c26667\c0\c60000;\cssrgb\c10196\c10196\c10196;\cssrgb\c7843\c34902\c61569;
\cssrgb\c27451\c27451\c27451;\cssrgb\c0\c0\c0;\cssrgb\c46275\c72549\c0;\cssrgb\c32157\c32157\c32157;\cssrgb\c95686\c95686\c95686;
\cssrgb\c94118\c94118\c94118;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid402\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}}
\margl1440\margr1440\vieww33140\viewh19980\viewkind0
\deftab720
\pard\pardeftab720\sa432\partightenfactor0

\f0\fs36 \cf2 \cb3 \expnd0\expndtw0\kerning0
Graphics giant Nvidia will no longer be spending $40 billion to acquire Arm, according to three anonymous sources \'93with direct knowledge of the transaction\'94 who {\field{\*\fldinst{HYPERLINK "https://www.ft.com/content/59c0d5f9-ed6a-4de6-a997-f25faed58833"}}{\fldrslt \cf4 spoke to the 
\f1\i Financial Times}}
\f1\i . 
\f0\i0 Instead, Nvidia will likely be spending up to $1.25 billion to Arm owner SoftBank for failing to go through with the transaction, while Arm CEO Simon Segars will reportedly lose his job in favor of Rene Hass (who coincidentally used to run Nvidia\'92s own Arm business many years ago).\cb1 \
\cb3 The deal, {\field{\*\fldinst{HYPERLINK "https://www.theverge.com/2020/9/13/21435507/nvidia-acquiring-arm-40-billion-chips-ai-deal"}}{\fldrslt \cf4 announced in September 2020}}, would have been one of the industry\'92s biggest ever, giving GPU manufacturer Nvidia control of the company whose architecture and intellectual property is key to every practically every smartphone and tablet chip ever made, as well as a growing number of server chips, and Apple\'92s entire future product roadmap for its impressive Arm-powered laptop and desktop PCs. ({\field{\*\fldinst{HYPERLINK "https://www.theverge.com/2020/11/19/21574057/apple-m1-chips-laptop-performance-intel-qualcomm-competition"}}{\fldrslt \cf4 Apple\'92s Arm laptop chips wowed the industry in late 2020}}, putting other manufacturers on notice.)\cb1 \
\cb3 Nvidia declined to comment to 
\f1\i The Verge
\f0\i0 , not even to confirm or deny that the deal had fallen through. Softbank and Arm didn\'92t immediately respond to requests for comment. None of the companies have publicly confirmed that the deal is off; it\'92s possible they are waiting until financial markets open in the UK or Japan, where Arm and Softbank are headquartered respectively. Softbank is {\field{\*\fldinst{HYPERLINK "https://twitter.com/SoftBank_Group/status/1490842193132593152"}}{\fldrslt \cf4 scheduled to report earnings}} later today on Japan time.\
\
\pard\pardeftab720\partightenfactor0

\f2\b\fs42 \cf5 So what\cb1 \
\pard\pardeftab720\sa240\partightenfactor0

\f3\b0\fs32 \cf6 \cb3 Specifically, over the weekend 
\f4\i Investor's Business Daily
\f3\i0  noted how a whole series of {\field{\*\fldinst{HYPERLINK "https://www.fool.com/investing/stock-market/market-sectors/information-technology/semiconductor-stocks/"}}{\fldrslt \cf7 \ul \ulc7 semiconductor stocks}} delivered beat-and-raise reports last week, which seems a bullish signal for the sector as a whole.\cb1 \
\cb3 As IBD details, on Friday alone, chip companies 
\f2\b Power Integrations
\f3\b0 ,
\f2\b  Synaptics
\f3\b0 ,
\f2\b  Microchip Technology
\f3\b0 , and\'a0
\f2\b Skyworks Solutions
\f3\b0  all reported earnings. Power Integrations beat analyst earnings estimates by 11%, Synaptics by 5%, Microchip by 2.5%, and Skyworks by 1% -- but while the size of the beats varied, all four companies undeniably exceeded expectations. Moreover, all four companies also predicted similar earnings beats in the coming quarter, with Synaptics in particular forecasting sales growth of 43% year over year. \'a0\cb1 \
\pard\pardeftab720\partightenfactor0

\f2\b\fs42 \cf5 \cb3 Now what\cb1 \
\pard\pardeftab720\sa240\partightenfactor0

\f3\b0\fs32 \cf6 \cb3 Does this bullish performance, and do these bullish prognoses, for other chip stocks mean Nvidia stock will do similarly well? Of course not -- not necessarily, at least. But the uniformly better-than-expected results do appear to bode well for the health of the semiconductor industry.\cb1 \
\cb3 And with analysts forecasting Nvidia to grow its sales 48% in the coming quarter, and its profits by 58%, it's easy to see why investors would be optimistic about the stock today.\cb1 \
\
\
\
\pard\pardeftab720\partightenfactor0

\f2\b\fs42 \cf5 \cb3 Should you invest $1,000 in NVIDIA Corporation right now?\cb1 \
\pard\pardeftab720\sa240\partightenfactor0

\f3\b0\fs32 \cf6 \cb3 Before you consider NVIDIA Corporation, you'll want to hear this.\cb1 \
\cb3 Our award-winning analyst team just revealed what they believe are the {\field{\*\fldinst{HYPERLINK "https://api.fool.com/infotron/infotrack/click?apikey=35527423-a535-4519-a07f-20014582e03e&impression=1d401a0f-9037-4186-a731-49913b6eb808&url=https%3A%2F%2Fwww.fool.com%2Fmms%2Fmark%2Fe-foolcom-sa-bbn-dyn%3Faid%3D8867%26source%3Disaeditxt0000713%26company%3DNVIDIA%2520Corporation%26ftm_mes%3Dfoolcom-sa-bbn-dyn-NVIDIA%2520Corporation%26ftm_cam%3Dsa-bbn-evergreen%26ftm_veh%3Darticle_pitch%26ftm_pit%3D9316"}}{\fldrslt 
\f2\b \cf7 \ul \ulc7 10 best stocks}} for investors to buy right now... and NVIDIA Corporation wasn't one of them.\cb1 \
\cb3 The online investing service they've run for nearly two decades, 
\f4\i Motley Fool Stock Advisor
\f3\i0 , has beaten the stock market by over 4X.* And right now, they think there are 10 stocks that are better buys.\
\
\pard\pardeftab720\sa320\partightenfactor0

\f5\b\fs36 \cf8 NVIDIA Corporation
\f6\b0 , global corporation that manufactures graphics processors, mobile technologies, and desktop computers. The company was founded in 1993 by three American computer scientists, Jen-Hsun Huang, Curtis Priem, and Christopher Malachowsky. NVIDIA is known for developing {\field{\*\fldinst{HYPERLINK "https://www.britannica.com/technology/integrated-circuit"}}{\fldrslt \cf9 integrated circuits}}, which are used in everything from {\field{\*\fldinst{HYPERLINK "https://www.britannica.com/topic/electronic-game"}}{\fldrslt \cf9 electronic game}} consoles to {\field{\*\fldinst{HYPERLINK "https://www.britannica.com/technology/personal-computer"}}{\fldrslt \cf9 personal computers}} (PCs). The company is a leading manufacturer of high-end graphics processing units (GPUs). NVIDIA is headquartered in {\field{\*\fldinst{HYPERLINK "https://www.britannica.com/place/Santa-Clara-California"}}{\fldrslt \cf9 Santa Clara}}, California.\cb1 \
\cb3 NVIDIA became a major force in the computer gaming industry with the launch of the RIVA series of graphics processors in 1997. Two years later, the company gained prominence with the release of the GeForce 256 GPU, which offered superior three-dimensional graphics quality. NVIDIA battled with prominent {\field{\*\fldinst{HYPERLINK "https://www.britannica.com/technology/video-card"}}{\fldrslt \cf9 video card}} maker 3dfx Interactive, pitting the GeForce against 3dfx Interactive\'92s popular Voodoo technologies. NVIDIA eventually prevailed and purchased 3dfx Interactive\'92s remaining assets in 2000. That same year, {\field{\*\fldinst{HYPERLINK "https://www.britannica.com/topic/Microsoft-Corporation"}}{\fldrslt \cf9 Microsoft Corporation}} selected NVIDIA to develop graphics cards for Microsoft\'92s long-awaited {\field{\*\fldinst{HYPERLINK "https://www.britannica.com/technology/Xbox"}}{\fldrslt \cf9 Xbox}} video game console. In 2007, NVIDIA was honoured as Company of the Year by 
\f7\i Forbes
\f6\i0  magazine for its rapid growth and success.\
\
\pard\pardeftab720\sa562\partightenfactor0

\f8\fs32 \cf10 Every living cell contains its own bustling microcosm, with thousands of components responsible for energy production, protein building, gene transcription and more.\cf0 \cb1 \
\cf10 \cb3 Scientists at the University of Illinois at Urbana-Champaign have built a 3D simulation that replicates these physical and chemical characteristics at a particle scale \'97 creating a fully dynamic model that mimics the behavior of a living cell.\cf0 \cb1 \
\pard\pardeftab720\sa562\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://doi.org/10.1016/j.cell.2021.12.025"}}{\fldrslt \cf12 \cb3 Published in the journal 
\f9\i Cell}}
\f9\i \cf10 \cb3 , 
\f8\i0 the project simulates a living minimal cell, which contains a pared-down set of genes essential for the cell\'92s survival, function and replication. The model uses NVIDIA GPUs to simulate 7,000 genetic information processes over a 20-minute span of the cell cycle \'96 making it what the scientists believe is the longest, most complex cell simulation to date.\cf0 \cb1 \
\pard\pardeftab720\sa562\partightenfactor0
\cf10 \cb3 Minimal cells are simpler than naturally occurring ones, making them easier to recreate digitally.\cf0 \cb1 \
\cf10 \cb3 \'93Even a minimal cell requires 2 billion atoms,\'94 said Zaida Luthey-Schulten, chemistry professor and co-director of the university\'92s Center for the Physics of Living Cells. \'93You cannot do a 3D model like this in a realistic human time scale without GPUs.\'94\cf0 \cb1 \
\cf10 \cb3 Once further tested and refined, whole-cell models can help scientists predict how changes to the conditions or genomes of real-world cells will affect their function. But even at this stage, minimal cell simulation can give scientists insight into the physical and chemical processes that form the foundation of living cells.\cf0 \cb1 \
\cf10 \cb3 \'93What we found is that fundamental behaviors emerge from the simulated cell \'97 not because we programmed them in, but because we had the kinetic parameters and lipid mechanisms correct in our model,\'94 she said.\cf0 \cb1 \
\pard\pardeftab720\sa562\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://ngc.nvidia.com/catalog/containers/hpc:lattice-microbes"}}{\fldrslt \cf12 \cb3 Lattice Microbes}}\cf10 \cb3 , the GPU-accelerated software co-developed by Luthey-Schulten and used to simulate the 3D minimal cell, is available on the {\field{\*\fldinst{HYPERLINK "https://ngc.nvidia.com/"}}{\fldrslt \cf12 NVIDIA NGC}} software hub.\cf0 \cb1 \
\pard\pardeftab720\sa300\partightenfactor0

\f10\b\fs39 \cf13 \cb3 Minimal Cell With Maximum Realism
\f8\b0 \cb1 \
\pard\pardeftab720\sa562\partightenfactor0

\fs32 \cf10 \cb3 To build the living cell model, the Illinois researchers simulated one of the simplest living cells, a parasitic bacteria called mycoplasma. They based the model on a trimmed-down version of a mycoplasma cell synthesized by scientists at J. Craig Venter Institute in La Jolla, Calif., which had just under 500 genes to keep it viable.\cf0 \cb1 \
\cf10 \cb3 For comparison, a single E. coli cell has around 5,000 genes. A human cell has more than 20,000.\cf0 \cb1 \
\cf10 \cb3 Luthy-Schulten\'92s team then used known properties of the mycoplasma\'92s inner workings, including amino acids, nucleotides, lipids and small molecule metabolites to build out the model with DNA, RNA, proteins and membranes.\cf0 \cb1 \
\cf10 \cb3 \'93We had enough of the reactions that we could reproduce everything known,\'94 she said.\cf0 \cb1 \
\cf10 \cb3 Using Lattice Microbes software on {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/data-center/tensor-cores/"}}{\fldrslt \cf12 NVIDIA Tensor Core GPUs}}, the researchers ran a 20-minute 3D simulation of the cell\'92s life cycle, before it starts to substantially expand or replicate its DNA. The model showed that the cell dedicated most of its energy to transporting molecules across the cell membrane, which fits its profile as a parasitic cell.\cf0 \cb1 \
\cf10 \cb3 \'93If you did these calculations serially, or at an all-atom level, it\'92d take years,\'94 said graduate student and paper lead author Zane Thornburg. \'93But because they\'92re all independent processes, we could bring parallelization into the code and make use of GPUs.\'94\cf0 \cb1 \
\cf10 \cb3 Thornburg is working on another GPU-accelerated project to simulate growth and cell division in 3D. The team has recently adopted {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/data-center/dgx-systems/"}}{\fldrslt \cf12 NVIDIA DGX systems}} and {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/design-visualization/rtx-a5000/"}}{\fldrslt \cf12 RTX A5000 GPUs}} to further accelerate its work, and found that using A5000 GPUs sped up the benchmark simulation time by 40 percent compared to a development workstation with a previous-generation NVIDIA GPU.\
\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 \cb1 GPU-Accelerated Asymmetric Numeral Systems with nvCOMP v2.2.0\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 The redesigned nvCOMP 2.2.0 interface provides a single nvcompManagerBase object that can do compression and decompression. Users can now decompress nvcomp-compressed files without knowing how they were compressed. The interface also can manage scratch space and split the input buffer into independent chunks for parallel processing.\
\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Learn to Deploy a Text Classification Model Using Riva (DLI)\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 This free, 30 minute, online course is self paced and includes a sample notebook from the NGC TAO Toolkit\'97Conversational AI collection, complete with a live GPU environment.\
\
In this free one-hour course, participants will work through a demonstration of a common vehicle routing optimization problem at their own pace. Upon completion, participants will be able to preprocess input data for use by NVIDIA ReOpt routing solver, and compose variants of the problem that reflect real-world business constraints\
\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Fundamentals of Accelerated Computing with CUDA Python (DLI)\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 This Deep Learning Institute workshop teaches you the fundamental tools and techniques for running GPU-accelerated Python applications using CUDA GPUs and the Numba compiler. This workshop is being offered Feb, 23 from 9 am to 5 pm PT.\
At the conclusion of the workshop, you\'92ll have an understanding of the fundamental tools and techniques for GPU-accelerated Python applications with CUDA and Numba, including:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
GPU-accelerate NumPy ufuncs with a few lines of code.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Configure code parallelization using the CUDA thread hierarchy.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Write custom CUDA device kernels for maximum performance and flexibility.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Use memory coalescing and on-device shared memory to increase CUDA kernel bandwidth\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
whereas in floating-point numbers the decimal \'93point\'94 can float (move).\
\pard\pardeftab720\sa320\partightenfactor0
\ls1\ilvl0\cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
The basic idea behind fixed-point numbers is that even though the values being represented can have fractional digits (aka the 0.23 in 1.23), you actually store the value as an integer. To represent 1.23, for example, you can construct a 
\f11 \cf8 \cb14 fixed_point
\f8 \cf8 \cb1  number with 
\f11 \cf8 \cb14 scale = -2
\f8 \cf8 \cb1  and value 123. This representation can be converted to a floating point number by multiplying the value by the radix raised to the scale. So in our example, 1.23 is produced by multiplying 123 (value) by 0.001 (10 (radix) to the power of -2 (scale)). When constructing a fixed-point number, the opposite occurs and you \'93shift\'94 the value so that you can store it as an integer (with the floating point number 1.23 you would divide by 0.001 if you were using scale -2 (0.001 (10 (radix) to the power of -2 (scale))).\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Note that fixed-point representations are not unique because you can choose multiple scales. For the example of 1.23, you can use any scale less than -2, such as -3 or -4. The only difference is that the number stored on disk will be different; 123 for scale -2, 1230 for scale -3 and 12300 for scale -4. When you know that your use case only requires a set number of decimal places, you should use the 
\f9\i \cf8 least precise
\f8\i0 \cf8  (aka largest) scale possible to maximize the range of representable values. With scale -2 the range is roughly -20 to +20 million (with two decimal places), whereas with scale -3 the range is roughly -2 to +2 million (with three decimal places). If you know you are modeling money and you don\'92t need three decimal places, scale -2 is a much better option.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Another parameter of a fixed-point type is the base. In the examples in this post, and in RAPIDS cuDF, we use base 10, or decimal fixed point. Decimal fixed point is the easiest to think about because we are comfortable with decimal (base 10) numbers. The other common base for fixed-point numbers is base 2, also known as binary fixed point. This simply means that instead of shifting value by powers of 10, the scale shifts a value by powers of 2. You can see some examples of binary and decimal fixed-point values later in the \'93Examples\'94 section.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
In NVIDIA CUDA 11.5, the NVCC offline compiler has added preview support for the signed and unsigned 
\f11 \cf8 \cb14 __int128
\f8 \cf8 \cb1  data types on platforms where the host compiler supports it. The 
\f11 \cf8 \cb14 nvrtc
\f8 \cf8 \cb1  JIT compiler has also added support for 128-bit integers, but requires a command-line option, 
\f11 \cf8 \cb14 --device-int128
\f8 \cf8 \cb1 , to enable this support.\'a0Arithmetic, logical, and bitwise operations are all supported on 128-bit integers. Note that DWARF debug support for 128-bit integers is not available yet and will be available in a subsequent CUDA release. With the 11.6 release, cuda-gdb and Nsight Visual Studio Code Edition have added support for inspecting this new variable type.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
NVIDIA GPUs compute integers in 32-bit quantities, so 128-bit integers are represented using four 32-bit unsigned integers. The addition, subtraction, and multiplication algorithms are straightforward and use the built-in PTX addc/madc instructions to handle multiple-precision values. Division and remainder are implemented using a simple O(n^2) division algorithm, similar to Algorithm 1.6 in Brent and Zimmermann\'92s book 
\f9\i \cf8 Modern Computer Arithmetic
\f8\i0 \cf8 , with a few optimizations to improve the quotient selection step and minimize correction steps.One of the motivating use cases for 128-bit integers is using them to implement decimal fixed-point arithmetic. 128-bit decimal fixed-point support is included in the 21.12 release of {\field{\*\fldinst{HYPERLINK "https://github.com/rapidsai/cudf"}}{\fldrslt \cf12 RAPIDS libcudf}}. Keep reading to find out more about fixed-point arithmetic and how 
\f11 \cf8 \cb14 __int128
\f8 \cf8 \cb1  is used to enable high-precision computation.\
\pard\pardeftab720\sa320\partightenfactor0
\ls1\ilvl0
\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Fixed-point Arithmetic\
\pard\pardeftab720\sa320\partightenfactor0
\ls1\ilvl0
\fs36 \AppleTypeServices \cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Fixed-point numbers represent real numbers by storing a fixed number of digits for the fractional part. Fixed-point numbers can also be used to \'93omit\'94 the lower-order digits of integer values (i.e. if you want to represent multiples of 1000, you can use a base-10 fixed-point number with scale equal to 3). One easy way to remember the difference between fixed-point and floating point is that with fixed-point numbers, the decimal \'93point\'94 is fixed,\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
representation of real numbers, implemented in many processors, including GPUs. It is popular due to its ability to represent a large dynamic range of values and to trade off range and precision.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Unfortunately, floating point\'92s flexibility and range can cause trouble in applications where accuracy within a fixed range is more important: think dollars and cents. Binary floating point numbers cannot exactly represent every decimal value, and their approximation and rounding can lead to accumulation of errors that may be unacceptable in accounting calculations, for example. Moreover, adding very large and very small floating-point numbers can result in truncation errors. For these reasons, many currency and accounting computations are implemented using fixed-point decimal arithmetic, which stores a fixed number of fractional digits. Depending on the range required, fixed-point arithmetic may need a larger number of bits.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
NVIDIA GPUs do not implement fixed-point arithmetic in hardware, but a GPU-accelerated software implementation can be efficient. In fact, RAPIDS cuDF library has provided efficient 32- and 64-bit fixed-point decimal numbers and computation for a while now. But some users of RAPIDS cuDF and GPU-accelerated Apache Spark need the higher range and precision provided by 128-bit decimals, and now NVIDIA CUDA 11.5 provides preview support of the 128-bit integer type
\f11 \cf8 \cb14 (int128)
\f8 \cf8 \cb1  that is needed to implement 128-bit decimal arithmetic.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
In this post, after introducing CUDA\'92s new int128 support, we detail how we implemented decimal fixed-point arithmetic on top of it. We then demonstrate how 128-bit fixed-point support in RAPIDS cuDF enables key Apache Spark workloads to run entirely on GPU.\
\pard\pardeftab720\sa320\partightenfactor0
\ls1\ilvl0
\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Introducing CUDA 
\f11 \AppleTypeServices \cf12 \cb14 __int128
\f8 \AppleTypeServices\AppleTypeServicesF65539 \cf12 \cb1 \
\pard\pardeftab720\sa320\partightenfactor0
\ls1\ilvl0
\fs36 \AppleTypeServices \cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}
\fs32 \cf8 \expnd0\expndtw0\kerning0
\
\ls1\ilvl0
\f9\i\fs36 \cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\'93Truth is much too complicated to allow anything but approximations.
\f8\i0 \cf8 \'94 \'97 
\f10\b \cf8 John von Neumann
\f8\b0 \cf8 \
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
The history of computing has demonstrated that there is no limit to what can be achieved with the relatively simple arithmetic implemented in computer hardware. But the \'93truth\'94 that computers represent using finite-size numbers is fundamentally approximate. As David Goldberg wrote, \'93{\field{\*\fldinst{HYPERLINK "https://docs.google.com/document/d/1uPOCWrEkUIYjorZEzYruNVS0l0P_MiCyVKb6xh9yUF8/edit#heading=h.tn56yggc97mm"}}{\fldrslt \cf12 Squeezing infinitely many real numbers into a finite number of bits requires an approximate representation}}.\'94 Floating point is the most widely used\
\pard\pardeftab720\sa320\partightenfactor0
\cf8 In contrast, when using fixed-point representations, an integer is used to store the 
\f10\b \cf8 exact 
\f8\b0 \cf8 value. To represent 1.1 using a fixed-point number with a scale equal to -1, the value 11 is stored.\'a0 Arithmetic operations are performed on the underlying integer, so adding 1.1 + 1.1 as fixed-point numbers simply adds 11 + 11 yielding 22, representing the value 2.2 exactly\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Why is fixed-point arithmetic important?\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 As shown in the example preceding, fixed-point arithmetic avoids the precision and rounding errors inherent in floating point numbers while still providing the ability to represent fractional digits. Floating point provides a much larger range of values by keeping relative error constant. However, this means it can suffer from large absolute (truncation) errors when adding very large and very small numbers and run into rounding errors. Fixed-point representation always has the same 
\f10\b \cf8 absolute
\f8\b0 \cf8  error at the cost of being able to represent a reduced range of values. If you know you need a specific precision after the decimal/binary point, then fixed point allows you to maintain accuracy of those digits without truncation even as the value grows, up to the limits of the range. If you need more range, you have to add more bits. Hence decimal128 becomes important for some users.\
\
Powerful airborne sensors could be key in helping farmers sustainably manage maize across the US Corn Belt, according to a University of Illinois research team. The {\field{\*\fldinst{HYPERLINK "https://www.sciencedirect.com/science/article/pii/S030324342100324X?via%3Dihub"}}{\fldrslt \cf12 study}}, which employs remote sensors combined with newly developed deep learning models, gives an accurate and speedy prediction of crop nitrogen, chlorophyll, and photosynthetic capacity.\
Published in the 
\f9\i \cf8 International Journal of Applied Earth Observation and Geoinformation
\f8\i0 \cf8 , the work could guide farmer management practices, helping reduce fertilizer use, boost food production, and alleviate environmental damage across the region.\
\'93Compared to the conventional approaches of leaf tissue analysis, remote sensing provides much faster and more cost-effective approaches to monitor crop nutrients. The timely and high-resolution crop nitrogen information will be very helpful to growers to diagnose crop growth and guide adaptive management,\'94 said lead author {\field{\*\fldinst{HYPERLINK "https://shengwang12.github.io/"}}{\fldrslt \cf12 Sheng Wang}}, a research scientist and assistant professor at the University of Illinois Urbana-Champaign.\
Producing about 75% of corn in the US and 30% globally, the Corn Belt plays a major role in food production. Extending from Indiana to Nebraska, the region yields 20 times more than it did in the 1880s, thanks to improved farming, corn breeding, new technologies, and fertilizers.\
Farmers rely on nitrogen-based fertilizers to boost photosynthesis, crop yields, and biomass for bioenergy crops. However, excessive application degrades soil, pollutes water sources, and contributes to global warming\'97nitrogen represents one of the largest sources of greenhouse gas emissions in agriculture.\'a0\
Accurately measuring nitrogen levels in crops could help farmers avoid over application, but manually conducting surveys is time-consuming and labor-intensive.\'a0\
\'93Precision agriculture that relies on advanced sensing technologies and airborne satellite platforms to monitor crops could be the solution,\'94 said project lead {\field{\*\fldinst{HYPERLINK "http://faculty.nres.illinois.edu/~kaiyuguan/"}}{\fldrslt \cf12 Kaiyu Guan}}, the Blue Waters Associate Professor at the University of Illinois Urbana-Champaign.\
Up until now, there has not been a reliable method for quickly monitoring leaf nitrogen levels over the course of a growing season. Using hyperspectral imaging and machine-learning models, the team proposed a hybrid approach to address these limitations.\
Hyperspectral imaging\'97an expanding area of remote sensing\'97uses a spectrometer that breaks down a pixel into hundreds of images at different wavelengths, providing more information on the captured image.\'a0\
Equipped with a highly sensitive hyperspectral sensor, the researchers conducted plane surveys over an experimental field in Illinois, collecting crop reflectance data. Plant chemical composition, such as nitrogen and chlorophyll influences reflection, which the sensors can detect even in subtle wavelength changes of just 3 to 5 nanometers.\'a0\

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalt\clvertalbase \clcbpat15 \clwWidth3065\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt240 \clpadl240 \clpadb240 \clpadr240 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0

\fs32 \cf8 \cell \lastrow\row
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0
\f11\fs36 \cf8 \cb14 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Rep:
\f8 \cb1  the representation of the 
\f11 \cb14 fixed_point
\f8 \cb1  number (for example, the integer type used)\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0
\f11 \cf8 \cb14 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Rad:
\f8 \cb1  the 
\f11 \cb14 Radix
\f8 \cb1  of the number (for example, base 2 or base 10)\
\pard\pardeftab720\sa320\partightenfactor0
\cf8 The 
\f11 \cf8 \cb14 decimal32
\f8 \cf8 \cb1  and 
\f11 \cf8 \cb14 decimal64
\f8 \cf8 \cb1  types use 
\f11 \cf8 \cb14 int32_t
\f8 \cf8 \cb1  and 
\f11 \cf8 \cb14 int64_t
\f8 \cf8 \cb1  for the Rep, respectively and both have 
\f11 \cf8 \cb14 Radix::BASE_10.
\f8 \cf8 \cb1  The 
\f11 \cf8 \cb14 scale
\f8 \cf8 \cb1  is a strongly typed run-time variable (see Run-Time Scale and Strong Typing subsections below, etc).\
The 
\f11 \cf8 \cb14 fixed_point
\f8 \cf8 \cb1  type has several constructors (see Ways to Construct subsection below), explicit conversion operators to cast to both integral and floating point types, and a full complement of operators (addition, subtraction, etc.).\
\pard\pardeftab720\sa320\partightenfactor0

\fs48 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Sign of Scale\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 In most C++ fixed-point implementations (including RAPIDS libcudf\'92s), a 
\f10\b \cf8 negative scale
\f8\b0 \cf8  indicates the number of fractional digits. A 
\f10\b \cf8 positive scale
\f8\b0 \cf8  indicates the multiple that is representable (for example, if 
\f11 \cf8 \cb14 scale = 2
\f8 \cf8 \cb1  for a 
\f11 \cf8 \cb14 decimal32
\f8 \cf8 \cb1 , multiples of 100 can be represented).\
DecimalType in Apache Spark SQL is a data type that can represent Java BigDecimal values. SQL queries operating on financial data make significant use of the decimal type. Unlike the RAPIDS libcudf implementation of fixed-point decimal numbers, the maximum precision possible for DecimalType in Spark is limited to 38 decimal. The scale, which is defined as the number of digits after the decimal point, is also capped at 38. This definition is the negative of the C++ scale. For example, a decimal value like 0.12345 has a scale of 5 in Spark but a scale of -5 in libcudf.\
Spark closely follows the Apache Hive specification on precision calculations for operations and provides options to the user to configure precision loss for decimal operations. Spark SQL is aggressive about promoting precision of the result column when performing operations like aggregation, windowing, casting and so on This behavior in and of itself is what makes decimal128 extremely relevant to real-world queries and answers the question: \'93Why do we need support for high-precision decimal columns?\'94. Consider the example below, specifically the hash aggregate, which has a multiplication expression involving a decimal64 column, price, and a non-decimal column, quantity. Spark first casts the non-decimal column to an appropriate decimal column. It then determines the result precision, which is greater than the input precision. Therefore, it is quite common for the result precision to be a decimal128 value even if decimal64 inputs are involved.\
The RAPIDS Spark plug-in is set up to run operators on the GPU only if all the expressions can be evaluated on the GPU. Let\'92s first look at the following physical plan for this query without decimal128 support.)\
Without decimal128 support every operator falls back to the CPU because child expressions that contain a decimal 128 type cannot be supported. Therefore, the containing exec or parent expression will also not execute on the GPU to avoid inefficient row-to-column and column-to-row conversions.\
\
Using {\field{\*\fldinst{HYPERLINK "https://www.sciencedirect.com/topics/earth-and-planetary-sciences/radiative-transfer"}}{\fldrslt \cf12 Radiative Transfer Modeling}} and a data-driven Partial-Least Squares Regression (PLSR) approach, the team developed deep learning models to predict crop traits based on the airborne reflectance data. According to the study, PLSR requires a relatively small size of label data for model training.\
The researchers trained their deep learning models using cuDNN and {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/data-center/v100/"}}{\fldrslt \cf12 NVIDIA V100 GPUs}} to predict crop nitrogen, chlorophyll, and photosynthetic capacity at both leaf and canopy levels.\
Testing the algorithms against ground-truth data, the models were about 85% accurate. The technique is fast, scanning fields in just a few seconds per acre. According to Wang, such technology can be very helpful to diagnose crop nitrogen status and yield potential.\
The ultimate goal of the work is to use satellite imagery for large-scale nitrogen monitoring across every field in the US Corn Belt and beyond.\'a0\
\'93We hope this technology can provide stakeholders timely information and advance growers\'92 management practices for sustainable agricultural practices,\'94 Guan said.\
\
\pard\pardeftab720\sa320\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://developer.nvidia.com/discover/artificial-neural-network"}}{\fldrslt \cf12 Artificial Neural Networks}} (ANN) are the fundamental building blocks of {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/glossary/data-science/artificial-intelligence/"}}{\fldrslt \cf12 Artificial Intelligence (AI)}} technology. ANNs are the basis of{\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/glossary/data-science/machine-learning/"}}{\fldrslt \cf12  machine-learning}} models; they simulate the process of learning identical to human brains. Simply put, ANNs give machines the capacity to accomplish human-like performance (and beyond) for specific tasks.\'a0This article aims to provide Data Scientists with the fundamental high-level knowledge of understanding the low-level operations involved in the functions and methods invoked when training an ANN.\
As Data Scientists, we aim to solve business problems by exposing patterns in data. Often, this is done using machine learning algorithms to identify patterns and predictions expressed as a model . Selecting the correct model for a particular use case, and tuning parameters appropriately requires a thorough understanding of the problem and underlying algorithm(s). An understanding of the problem domain and the algorithms are taken under consideration to ensure that we are using the models appropriately, and interpreting results correctly.\
This article introduces and explains gradient descent and backpropagation algorithms. These algorithms facilitate how ANNs learn from datasets, specifically where modifications to the network\'92s parameter values occur due to operations involving data points and neural network predictions.\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Building an intuition\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 Before we get into the technical details of this post, let\'92s look at how humans learn.\
The human brain\'92s learning process is complicated, and research has barely scratched the surface of how humans learn. However, the little that we do know is valuable and helpful for building models. Unlike machines, humans do not need a large quantity of data to comprehend how to tackle an issue or make logical predictions; instead, we learn from our experiences and mistakes.\
Humans learn through a process of synaptic plasticity.\'a0Synaptic plasticity is a term used to describe how new neural connections are formed and strengthened after gaining new information. In the same way that the connections in the brain are strengthened and formed as we experience new events, we train artificial neural networks by computing the errors of neural network predictions and strengthening or weakening internal connections between neurons based on these errors.\
\pard\pardeftab720\sa320\partightenfactor0

\f10\b\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Gradient Descent
\f8\b0 \AppleTypeServices\AppleTypeServicesF65539 \cf12 \
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 Gradient Descent is a standard optimization algorithm. It is frequently the first optimization algorithm introduced to train machine learning. Let\'92s dissect the term \'93Gradient Descent\'94 to get a better understanding of how it relates to machine learning algorithms.\
A gradient is a measurement that quantifies the steepness of a line or curve.\'a0Mathematically, it details the direction of the ascent or descent of a line. Descent is the action of going downwards. Therefore, the gradient descent algorithm quantifies downward motion based on the two simple definitions of these phrases.\
To train a machine learning algorithm, you strive to identify the weights and biases within the network that will help you solve the problem under consideration.\'a0For example, you may have a classification problem.\'a0When looking at an image, you want to determine if the image is of a cat or a dog.\'a0To build your model, you train your algorithm with training data with correctly labeled data samples of cats and dogs images.\
While the example described above is classification, the problem could be localization or detection. Nonetheless, how 
\f9\i \cf8 well
\f8\i0 \cf8  a neural network performs on a problem is modeled as a function, more specifically, a cost function; a cost or what is sometimes called a loss function measures how wrong a model is. The partial derivatives of the cost function influence the ultimate model\'92s weights and biases selected.\
\pard\pardeftab720\sa320\partightenfactor0

\f9\i \cf8 Gradient Descent is the algorithm that facilitates the search of parameters values that minimize the cost function towards a local minimum or optimal accuracy.
\f8\i0 \cf8 \
\pard\pardeftab720\sa320\partightenfactor0

\f10\b\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Cost functions, Gradient Descent and Backpropagation in Neural Networks
\f8\b0 \AppleTypeServices\AppleTypeServicesF65539 \cf12 \
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 Neural networks are impressive.\'a0Equally impressive is the capacity for a computational program to distinguish between images and objects within images without being explicitly informed of what features to detect.\
It is helpful to think of a neural network as a function that accepts inputs (data ), to produce an output prediction. The variables of this function are the parameters or weights of the neuron.\
\pard\pardeftab720\sa320\partightenfactor0

\f9\i \cf8 Therefore the key assignment to solving a task presented to a neural network will be to adjust the values of the weights and biases in a manner that approximates or best represents the dataset.
\f8\i0 \cf8 \
The image below depicts a simple neural network that receives input(X1, X2, X3, Xn), these inputs are fed forward to neurons within the layer containing weights(W1, W2, W3, Wn). The inputs and weights undergo a multiplication operation and the result is summed together by an adder(), and an activation function regulates the final output of the layer.\
\
The image above illustrates a simple neural network architecture of densely connected neurons that classifies images containing the digits 0-3. Each neuron in the output layer corresponds to a digit. The higher the activations of the connection to a neuron, the higher the probability outputted by the neuron. The probability corresponds to the likelihood that the digit fed forward through the network is associated with the activated neuron.\
When a \'913\'92 is fed forward through the network, we expect the connections (represented by the arrows in the diagram) responsible for classifying a \'913\'92 to have higher activation, which results in a higher probability for the output neuron associated with the digit \'913\'92.\
Several components are responsible for the activation of a neuron, namely biases, weights, and the previous layer activations. These specified components have to be iteratively modified for the neural network to perform optimally on a particular dataset.\
By leveraging a cost function such as \'91mean squared error\'92, we obtain information in relation to the error of the network that is used to propagate updates backwards through the network\'92s weights and biases.\
For completeness, below are examples of cost functions used within machine learning:\
We have covered how to improve neural networks\'92 performance through a technique that measures the network\'92s predictions. The rest of the content in this article focuses on the relationship between gradient descent, backpropagation, and cost function.\
The image in figure 3 illustrates a cost function plotted on the x and y-axis that hold values within the function\'92s parameter space. Let\'92s take a look at how neural networks learn by visualizing the cost function as an uneven surface plotted on a graph within the parameter spaces of the possible weight/parameters values.\
The blue points in the image above represent a step (evaluation of parameters values into the cost function) in the search for a local minimum. The lowest point of a modeled cost function corresponds to the position of weights values that results in the lowest value of the cost function. The smaller the cost function is, the better the neural network performs. Therefore, it is possible to modify the networks\'92 weights from the information gathered.\
Gradient descent is the algorithm employed to guide the pairs of values chosen at each step towards a minimum.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0\cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Local Minimum: The minimum parameter values within a specified range or sector of the cost function.\
\ls3\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Global Minimum: This is the smallest parameter value within the entire cost function domain.\
\pard\pardeftab720\sa320\partightenfactor0
\cf8 The gradient descent algorithm guides the search for values that minimize the function at a local/global minimum by calculating the gradient of a differentiable function and moving in the opposite direction of the gradient.\
Backpropagation is the mechanism by which components that influence the output of a neuron (bias, weights, activations) are iteratively adjusted to reduce the cost function. In the architecture of a neural network, the neuron\'92s input, including all the preceding connections to the neurons in the previous layer, determines its output.\
The critical mathematical process involved in backpropagation is the calculation of derivatives. The backpropagation\'92s operations calculate the partial derivative of the cost function with respect to the weights, biases, and previous layer activations to identify which values affect the gradient of the cost function.\
The minimization of the cost function by calculating the gradient leads to a local minimum. In each iteration or training step, the weights in the network are updated by the calculated gradient, alongside the 
\f9\i \cf8 learning rate
\f8\i0 \cf8 , which controls the factor of modification made to weight values. This process is repeated for each step to be taken during the training phase of a neural network. Ideally, the goal is to be closer to a local minimum after each step.\
The name \'93Backpropagation\'94 comes from the process\'92s literal meaning, which is \'93backwards propagation of errors\'94.\'a0The partial derivative of the gradient quantifies the error. By propagating the errors backwards through the network, the partial derivative of the gradient of the last layer (closest layer to the output layer) is used to calculate the gradient of the second to the last layer.\
The propagation of errors through the layers and the utilization of the partial derivative of the gradient from a previous layer in the current layer occurs until the first layer(closest layer to the input layer) in the network is reached.\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Summary\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 This is just a primer on the topic of gradient descent. There is a whole world of mathematics and calculus associated with the topic of gradient descent.\'a0\
Packages such as {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/glossary/data-science/tensorflow/"}}{\fldrslt \cf12 TensorFlow}}, {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/glossary/data-science/scikit-learn/"}}{\fldrslt \cf12 SciKit-Learn}}, {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/glossary/data-science/pytorch/"}}{\fldrslt \cf12 PyTorch}} often abstract the complexities of implementing training and optimization algorithms. Nevertheless, this does not relieve Data Scientists and ML practitioners of the requirement of understanding what occurs behind the scenes of these intelligent \'91black boxes.\'92\
Want to explore more maths associated with backpropagation? Below are some resources to aid in your exploration:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0\cf12 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://www.jeremyjordan.me/neural-networks-training/"}}{\fldrslt \expnd0\expndtw0\kerning0
Neural Networks: training with backpropagation}}\cf8 \expnd0\expndtw0\kerning0
\
\ls4\ilvl0\cf12 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://brilliant.org/wiki/backpropagation/"}}{\fldrslt \expnd0\expndtw0\kerning0
Backpropagation}}\cf8 \expnd0\expndtw0\kerning0
\
\ls4\ilvl0\cf12 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "http://neuralnetworksanddeeplearning.com/chap2.html"}}{\fldrslt \expnd0\expndtw0\kerning0
How the backpropagation algorithm works}}\cf8 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\sa320\partightenfactor0
\cf8 Dive deeper into the world of deep learning by exploring the variety of courses available at the {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-in/training/online/#:~:text=DEEP-,LEARNING,-ACCELERATED%20COMPUTING"}}{\fldrslt \cf12 Nvidia Deep Learning Institute}}.\
\
This past year, NVIDIA announced several major breakthroughs in conversational AI for building and deploying automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech (TTS) applications.\
To get developers started with some quick examples in a cloud GPU-accelerated environment, {\field{\*\fldinst{HYPERLINK "https://www.nvidia.com/en-us/training/"}}{\fldrslt \cf12 NVIDIA Deep Learning Institute (DLI)}} is offering three fast, free, self-paced courses.\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 What will you learn?\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 These instructional DLI courses give developers a taste of how to use modern tools to quickly create conversational AI and NLP GPU-accelerated applications.\'a0Learning objectives include:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf12 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://courses.nvidia.com/courses/course-v1:DLI+T-FX-02+V1/about"}}{\fldrslt \expnd0\expndtw0\kerning0
Train a Text Classification Model Using TAO Toolkit}}\cf8 \expnd0\expndtw0\kerning0
\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls5\ilvl1\cf8 \kerning1\expnd0\expndtw0 {\listtext	
\f12 \uc0\u9702 
\f8 	}\expnd0\expndtw0\kerning0
Train and fine-tune a BERT text classification model on the SST-2 dataset.\
\ls5\ilvl1\kerning1\expnd0\expndtw0 {\listtext	
\f12 \uc0\u9702 
\f8 	}\expnd0\expndtw0\kerning0
Run evaluation and inference on the model.\
\ls5\ilvl1\kerning1\expnd0\expndtw0 {\listtext	
\f12 \uc0\u9702 
\f8 	}\expnd0\expndtw0\kerning0
Export the model to ONNX format or Riva format for deployment.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf12 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://courses.nvidia.com/courses/course-v1:DLI+T-FX-03+V1/about"}}{\fldrslt \expnd0\expndtw0\kerning0
Deploy a Text Classification Model Using Riva}}\cf8 \expnd0\expndtw0\kerning0
\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls5\ilvl1\cf8 \kerning1\expnd0\expndtw0 {\listtext	
\f12 \uc0\u9702 
\f8 	}\expnd0\expndtw0\kerning0
Use Riva ServiceMaker to take a TAO-exported Riva model and convert it for final deployment.\
\ls5\ilvl1\kerning1\expnd0\expndtw0 {\listtext	
\f12 \uc0\u9702 
\f8 	}\expnd0\expndtw0\kerning0
Deploy the model(s) locally on the Riva Server.\
\ls5\ilvl1\kerning1\expnd0\expndtw0 {\listtext	
\f12 \uc0\u9702 
\f8 	}\expnd0\expndtw0\kerning0
Send inference requests from a demo client using Riva API bindings.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf12 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://courses.nvidia.com/courses/course-v1:DLI+T-FX-04+V1/about"}}{\fldrslt \expnd0\expndtw0\kerning0
Riva Speech API Demo}}\cf8 \expnd0\expndtw0\kerning0
\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls5\ilvl1\cf8 \kerning1\expnd0\expndtw0 {\listtext	
\f12 \uc0\u9702 
\f8 	}\expnd0\expndtw0\kerning0
Send audio to an ASR model and receive back text.\
\ls5\ilvl1\kerning1\expnd0\expndtw0 {\listtext	
\f12 \uc0\u9702 
\f8 	}\expnd0\expndtw0\kerning0
Use NLP models to transform text, classify text, and classify tokens.\
\ls5\ilvl1\kerning1\expnd0\expndtw0 {\listtext	
\f12 \uc0\u9702 
\f8 	}\expnd0\expndtw0\kerning0
Send text to a TTS model and receive back audio.\
\pard\pardeftab720\sa320\partightenfactor0
\cf8 Upon course completion, developers will be familiar with:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls6\ilvl0\cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
How to train, infer, and export a text classification model using NVIDIA TAO Toolkit on NVIDIA GPUs.\
\ls6\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
How to deploy a text classification model using NVIDIA Riva on NVIDIA GPUs.\
\ls6\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
How to construct requests to an NVIDIA Riva Speech server from a sample client.\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Why is text classification useful?\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 Text classification answers the question: Which category does this bit of text belong in? For example, if you want to know whether a movie review is positive or negative, you can use two categories to build a sentiment analysis project.\
Take this one step further, and classify sentences or documents by topic using several categories. In both use cases, you start with a pre-trained language model and then \'93train\'94 a classifier using example classified text to create our text classification project.\
Granted, text classification is just one of many NLP tasks that uses a pre-trained language model to understand written language.\'a0Once developers try NVIDIA TAO Toolkit and NVIDIA Riva to train and deploy {\field{\*\fldinst{HYPERLINK "https://docs.nvidia.com/tao/tao-toolkit/text/nlp/text_classification.html"}}{\fldrslt \cf12 text classification}} projects, they will be in a position to extend that experience to additional NLP tasks, such as {\field{\*\fldinst{HYPERLINK "https://docs.nvidia.com/tao/tao-toolkit/text/nlp/token_classification.html"}}{\fldrslt \cf12 named entity recognition (NER)}} and {\field{\*\fldinst{HYPERLINK "https://docs.nvidia.com/tao/tao-toolkit/text/nlp/question_answering.html"}}{\fldrslt \cf12 question answering}}.\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 How does the NVIDIA Riva Speech API work?\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 The Riva Speech API server exposes a simple API for performing speech recognition, speech synthesis, and a variety of NLP inferences. In this course, developers use Python examples to run several of these API calls from within a Riva sample client.\'a0The server is prepopulated with ASR, NLP, and TTS models. These built-in models allow developers to test several conversational AI components quickly with ease.\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Start learning about NLP and conversational AI\
\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 This month, NVIDIA released world-class speech-to-text models for Spanish, German, and Russian in {\field{\*\fldinst{HYPERLINK "https://developer.nvidia.com/riva"}}{\fldrslt \cf12 Riva}}, powering enterprises to deploy speech AI applications globally. In addition, enterprises can now create expressive speech interfaces using Riva\'92s customizable text-to-speech pipeline.\
NVIDIA Riva is a GPU-accelerated speech AI SDK for developing real-time applications like live captioning, adding voice to text-based chatbots, and generating real-time transcription in call centers. For easy implementation, Riva offers highly accurate pretrained models in the {\field{\*\fldinst{HYPERLINK "https://catalog.ngc.nvidia.com/models?filters=&orderBy=dateModifiedDESC&query=riva"}}{\fldrslt \cf12 NGC}} catalog.\'a0\
With the {\field{\*\fldinst{HYPERLINK "https://developer.nvidia.com/tao-toolkit"}}{\fldrslt \cf12 TAO Toolkit}}, these models can be customized for any industry including telecommunications, finance, unified communications as a service, and healthcare. Developers can use Riva to deploy these models out-of-the-box. They are optimized to run in real time in less than 300 ms in the cloud, data center, and at the edge.\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Riva release highlights include\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0
\fs36 \AppleTypeServices \cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
World-class speech recognition skills in Spanish, German, and Russian.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0\cf8 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Customizable text-to-speech pipeline for expressive interactions.\
\ls7\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Low-code fine-tuning workflow with TAO Toolkit.\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Automatic speech recognition in multiple languages\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 Every conversational AI application, from call centers to virtual assistants, relies heavily on automatic speech recognition. Enterprises can extend these apps globally with Riva automatic speech recognition in English, Spanish, German, and Russian.\
The non-English automatic speech recognition models are trained on a variety of open-source datasets, such as Mozilla Common Voice, as well as private datasets. Riva automatic speech recognition models are developed to provide out-of-the-box accuracy and serve as a great starting point for adapting to industry, jargon, dialect, or even noisy surroundings. On popular evaluation datasets, these models deliver world-class accuracy on several industry applications.\
\pard\pardeftab720\sa320\partightenfactor0

\fs64 \AppleTypeServices\AppleTypeServicesF65539 \cf12 Customizable text-to-speech pipelines\
\pard\pardeftab720\sa320\partightenfactor0

\fs36 \AppleTypeServices \cf8 For customers to enjoy lifelike dialogues, speech applications must offer human-like expressions. Using Fastpitch, a new model created by the NVIDIA speech AI research team, Riva helps developers customize the text-to-speech pipeline and create expressive speech interfaces. For example, during inference time, developers can vary voice pitch and speed using SSML tags.\'a0\
}